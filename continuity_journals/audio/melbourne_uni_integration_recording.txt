INTEGRATION WITH MELBOURNE UNI STUDENTS: VOICE RECORDER PROJECT

0. Overview

This document, initially prepared by Manny Rayner and ChatGPT
C-LARA-Instance, provides information needed to organise the
integration of the Voice Recorder project into C-LARA. On the C-LARA
side, we have filled in what look to us like the key points.
There are placeholders for information that the Melbourne Uni
Voice Recorder team will need to provide.

The document is placed in the GitHub repo at
https://github.com/mannyrayner/C-LARA/blob/main/continuity_journals/audio/melbourne_uni_integration_recording.txt, where ChatGPT can read it.

When we have completed the document and resolved outstanding issues,
the intention is that it will serve as a template for similar documents
relating to the other Melbourne Uni projects.

1. Overview of Melbourne Uni Voice Recorder Work

To be supplied by Melbourne Uni students:

a. Brief description of current progress, functionalities, and design.

b. Challenges or issues faced.

c. Future plans for features that will be added.

2. Overview of C-LARA work

On the C-LARA side, we have started with a preliminary manual
integration based on the LiteDevTools (LDT) voice recorder used in the
previous LARA project. We have referred to LDT throughout, and our
understanding is that the Melbourne Uni voice recorder is quite
similar to it.

The manual integration is incorporated into the Heroku deployment of
C-LARA. The high-level workflow is as follows:

a. Create a C-LARA project. In the usual way, create plain text, then
add segmentation, gloss and lemma annotations.

b. Specify relevant parameters in the Human Audio Processing view.

c. Download metadata for segments/words.

d. Upload metadata to LDT.

e. Record audio on LDT.

f. Download results zipfile (audio + instantiated metadata) from LDT.

g. Upload results zipfile through Human Audio Processing view.

3. APIs, code, interfaces, examples

a. Details of current manual API

The Heroku deployment is at https://c-lara-758a4f81c1ff.herokuapp.com/

In the Human Audio Processing view for the project,
specify that you are doing audio recording, and whether
this applies to segments, words or both. Specify an ID
for the voice talent.

There are controls for downloading metadata and uploading an LDT zipfile.

b. Formal spec of current LDT-based data formats

- The downloaded metadata is a JSON-formatted file containing a
list of structures of the form

    {
        "text": "... the text ...",
        "file": "... the audio file ..."
    }

If there is no current audio file, "file" will be null.

- The uploaded results file is a zipfile containing the recorded mp3 and
instantiated metadata.

c. Toy example ("Mary had a little lamb")

The initial uninstantiated segment metadata looks like this:

[
    {
        "text": "Mary had a little lamb",
        "file": ""
    },
    {
        "text": "Its fleece was white as snow",
        "file": ""
    },
    {
        "text": "And everywhere that Mary went",
        "file": ""
    },
    {
        "text": "That lamb was sure to go",
        "file": ""
    }
]

The final instantiated metadata looks like this:

[
    {
        "text": "Mary had a little lamb",
        "file": "2676569_230925_141412968.mp3"
    },
    {
        "text": "Its fleece was white as snow",
        "file": "2676570_230925_141421014.mp3"
    },
    {
        "text": "And everywhere that Mary went",
        "file": "2676571_230925_141425357.mp3"
    },
    {
        "text": "That lamb was sure to go",
        "file": "2676572_230925_141432770.mp3"
    }
]

d. Details of immediately underlying code, which will become the API in the final version

- The code is in the project's GitHub repo, https://github.com/mannyrayner/C-LARA/tree/main

- The top-level Django view code is in
https://github.com/mannyrayner/C-LARA/blob/main/clara_app/views.py
Look in particular at the functions

human_audio_processing(request, project_id)

generate_audio_metadata(request, project_id, metadata_type, human_voice_id)

- The entry points for the main processing are in
https://github.com/mannyrayner/C-LARA/blob/main/clara_app/clara_core/clara_main.py. Look
in particular at the methods get_audio_metadata and get_audio_metadata

4. Developing a detailed integration spec

Immediate questions to resolve:

a. Will the data formats differ from those used in the preliminary LDT integration? If so, how?

b. What code will the Melbourne Uni team provide? What are the endpoints? How will it be added to the C-LARA codebase?

c. Will we start with a preliminary manual integration, uploading and downloading files through the C-LARA web interface as we do with LDT?

d. For the final automatic integration, what changes will we need to make in the C-LARA code to call the Melbourne Uni code?

5. Next Steps and Timeline:

a. Contact points for technical support or questions.

On the C-LARA side, please send mail to Manny.Rayner@unisa.edu.au
with a CC to chatgptclarainstance@proton.me

b. A timeline for the integration process.

To be discussed.

c. Milestones or checkpoints to track progress.

To be discussed.
